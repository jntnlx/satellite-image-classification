{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0231bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchviz\n",
    "import torchinfo\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2025c8a",
   "metadata": {},
   "source": [
    "**Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9b6b06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Raw source dataset: ../data/raw/eurosat\n",
      "Processed train data directory: ../data/processed/eurosat/train\n",
      "Processed test data directory: ../data/processed/eurosat/test\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "random.seed(42)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Split\n",
    "train_split_ratio = 0.8  # 80/20 Train/Test Ratio\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "\n",
    "# Pathing\n",
    "data_root = Path('../data')\n",
    "data_dir = data_root / 'raw' / 'eurosat'\n",
    "\n",
    "print(f\"Raw source dataset: {data_dir}\")\n",
    "\n",
    "processed_data_dir = data_root / 'processed' / 'eurosat'\n",
    "processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dir = processed_data_dir / 'train'\n",
    "test_dir = processed_data_dir / 'test'\n",
    "\n",
    "# Create Directories\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Processed train data directory: {train_dir}\")\n",
    "print(f\"Processed test data directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc53673",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e0727ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_progress(count, total, suffix=''):\n",
    "#     bar_len = 60\n",
    "#     filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "#     percents = round(100.0 * count / float(total), 1)\n",
    "#     bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "#     print(f'[{bar}] {percents}% ...{suffix}', end='\\r')\n",
    "\n",
    "# def show_images(images, nmax=64):\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     ax.set_xticks([]); ax.set_yticks([])\n",
    "#     ax.imshow(make_grid((images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "    \n",
    "# def show_batch(dl, nmax=64):\n",
    "#     for images in dl:\n",
    "#         show_images(images, nmax)\n",
    "#         break\n",
    "\n",
    "# show_batch(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc7a7d",
   "metadata": {},
   "source": [
    "**Data Pre-Processing: Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfa659d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Label Class Data:\n",
      "'SeaLake' (../data/raw/eurosat/SeaLake)\n",
      "'Highway' (../data/raw/eurosat/Highway)\n",
      "'HerbaceousVegetation' (../data/raw/eurosat/HerbaceousVegetation)\n",
      "'PermanentCrop' (../data/raw/eurosat/PermanentCrop)\n",
      "'Pasture' (../data/raw/eurosat/Pasture)\n",
      "'Forest' (../data/raw/eurosat/Forest)\n",
      "'Industrial' (../data/raw/eurosat/Industrial)\n",
      "'AnnualCrop' (../data/raw/eurosat/AnnualCrop)\n",
      "'Residential' (../data/raw/eurosat/Residential)\n",
      "'River' (../data/raw/eurosat/River)\n",
      "Done. Data Split Train (80%) / Test (20%) Ratio.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing Label Class Data:\")\n",
    "\n",
    "for class_dir in data_dir.iterdir():\n",
    "\n",
    "    print(f\"'{os.path.basename(class_dir)}' ({class_dir})\")\n",
    "\n",
    "    if class_dir.is_dir():\n",
    "\n",
    "        class_name = class_dir.name\n",
    "\n",
    "        # Get images in respective class directory\n",
    "        images = list(class_dir.glob('*.*'))  # EuroSAT file format: *.jpg\n",
    "\n",
    "        # Shuffle images for random split\n",
    "        random.shuffle(images)  \n",
    "\n",
    "        # Split\n",
    "        split_index = int(train_split_ratio * len(images))\n",
    "        train_images = images[:split_index]\n",
    "        test_images = images[split_index:]\n",
    "\n",
    "        # Class directories in train/test folder\n",
    "        (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        (test_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Move images\n",
    "        for image in train_images:\n",
    "\n",
    "            shutil.copy(image, train_dir / class_name / image.name)\n",
    "\n",
    "        for image in test_images:\n",
    "\n",
    "            shutil.copy(image, test_dir / class_name / image.name)\n",
    "\n",
    "print(f\"Done. Data Split Train ({100*train_split_ratio:.0f}%) / Test ({100*(1 - train_split_ratio):.0f}%) Ratio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d72f00",
   "metadata": {},
   "source": [
    "**Data Pre-Processing: Transformations (Augmentation) and DataLoaders (PyTorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03370ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 27000\n",
      "    Root location: ../data/processed/eurosat/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToImage()\n",
      "                 Resize(size=[64, 64], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 RandomHorizontalFlip(p=1)\n",
      "                 RandomVerticalFlip(p=1)\n",
      "                 RandomRotation(degrees=[0.0, 180.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
      "                 RandomApply(    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1)))\n",
      "                 ToDtype(scale=True)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 25151\n",
      "    Root location: ../data/processed/eurosat/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "                 ToImage()\n",
      "                 Resize(size=[64, 64], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "                 RandomHorizontalFlip(p=1)\n",
      "                 RandomVerticalFlip(p=1)\n",
      "                 RandomRotation(degrees=[0.0, 180.0], interpolation=InterpolationMode.NEAREST, expand=False, fill=0)\n",
      "                 RandomApply(    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1)))\n",
      "                 ToDtype(scale=True)\n",
      "                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Transformations (Data Augmentation)\n",
    "transformations = transforms.v2.Compose(  # Using `torchvision.transforms.v2` instead of `torchvision.transforms`\n",
    "    \n",
    "    # ref: https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py\n",
    "\n",
    "    [\n",
    "\n",
    "        transforms.v2.ToImage(),  # Convert to PIL Image or Tensor ()\n",
    "\n",
    "        # transforms.v2.Resize((64, 64)),  # EurosSAT dimensions\n",
    "        transforms.v2.RandomResizedCrop(size=(64, 64), scale=(0.8, 1.0), antialias=True),  # Potential robustness to scale/positon variation\n",
    "\n",
    "        transforms.v2.RandomHorizontalFlip(p=0.5),  # Flip horizontally\n",
    "        transforms.v2.RandomVerticalFlip(p=0.5),  # Flip vertically\n",
    "        transforms.v2.RandomRotation(degrees=(0, 180)),  # Rotation range 0 to 90 degrees\n",
    "\n",
    "        # ColorJitter: TUNE for realistic satallite image examples, i.e. atmospheric interference manifesting in images\n",
    "        transforms.v2.RandomApply([\n",
    "            transforms.v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "        ], p=0.8),\n",
    "        \n",
    "        # CAUTION: potential adverse effect on satellite imagery        \n",
    "        # transforms.v2.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "        # transforms.v2.gaussian_blur(kernel_size=(5, 9), sigma=(0.1, 1.0)),  \n",
    "\n",
    "        # transforms.v2.ToTensor(),  # DEPRECATED in v2 -> use `transforms.v2.ToImage()` ... `v2.ToDtype(torch.float32, scale=True)` instead\n",
    "        transforms.v2.ToDtype(torch.float32, scale=True),\n",
    "        transforms.v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Pre-Trained ImageNet (Standards)\n",
    "\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Load data\n",
    "training_data = datasets.ImageFolder(train_dir, transform=transformations)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=transformations)\n",
    "\n",
    "# Extract classes from directory structure\n",
    "num_classes = len(training_data.classes)\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    \n",
    "    training_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    "\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    \n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers\n",
    "\n",
    ")\n",
    "\n",
    "print(training_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf6779",
   "metadata": {},
   "source": [
    "**Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "751a22c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "model = models.resnet18(weights=weights)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training\n",
    "loss_fn = nn.CrossEntropyLoss()  # or `criterion`\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66106d9",
   "metadata": {},
   "source": [
    "**Visualize Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4db02187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape               Output Shape              Kernel Shape              Param #\n",
       "============================================================================================================================================\n",
       "ResNet (ResNet)                          [32, 3, 64, 64]           [32, 10]                  --                        --\n",
       "├─Conv2d (conv1)                         [32, 3, 64, 64]           [32, 64, 32, 32]          [7, 7]                    9,408\n",
       "├─BatchNorm2d (bn1)                      [32, 64, 32, 32]          [32, 64, 32, 32]          --                        128\n",
       "├─ReLU (relu)                            [32, 64, 32, 32]          [32, 64, 32, 32]          --                        --\n",
       "├─MaxPool2d (maxpool)                    [32, 64, 32, 32]          [32, 64, 16, 16]          3                         --\n",
       "├─Sequential (layer1)                    [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "│    └─BasicBlock (0)                    [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 64, 16, 16]          [32, 64, 16, 16]          [3, 3]                    36,864\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 64, 16, 16]          [32, 64, 16, 16]          --                        128\n",
       "│    │    └─ReLU (relu)                  [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 64, 16, 16]          [32, 64, 16, 16]          [3, 3]                    36,864\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 64, 16, 16]          [32, 64, 16, 16]          --                        128\n",
       "│    │    └─ReLU (relu)                  [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "│    └─BasicBlock (1)                    [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 64, 16, 16]          [32, 64, 16, 16]          [3, 3]                    36,864\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 64, 16, 16]          [32, 64, 16, 16]          --                        128\n",
       "│    │    └─ReLU (relu)                  [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 64, 16, 16]          [32, 64, 16, 16]          [3, 3]                    36,864\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 64, 16, 16]          [32, 64, 16, 16]          --                        128\n",
       "│    │    └─ReLU (relu)                  [32, 64, 16, 16]          [32, 64, 16, 16]          --                        --\n",
       "├─Sequential (layer2)                    [32, 64, 16, 16]          [32, 128, 8, 8]           --                        --\n",
       "│    └─BasicBlock (0)                    [32, 64, 16, 16]          [32, 128, 8, 8]           --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 64, 16, 16]          [32, 128, 8, 8]           [3, 3]                    73,728\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 128, 8, 8]           [32, 128, 8, 8]           --                        256\n",
       "│    │    └─ReLU (relu)                  [32, 128, 8, 8]           [32, 128, 8, 8]           --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 128, 8, 8]           [32, 128, 8, 8]           [3, 3]                    147,456\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 128, 8, 8]           [32, 128, 8, 8]           --                        256\n",
       "│    │    └─Sequential (downsample)      [32, 64, 16, 16]          [32, 128, 8, 8]           --                        8,448\n",
       "│    │    └─ReLU (relu)                  [32, 128, 8, 8]           [32, 128, 8, 8]           --                        --\n",
       "│    └─BasicBlock (1)                    [32, 128, 8, 8]           [32, 128, 8, 8]           --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 128, 8, 8]           [32, 128, 8, 8]           [3, 3]                    147,456\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 128, 8, 8]           [32, 128, 8, 8]           --                        256\n",
       "│    │    └─ReLU (relu)                  [32, 128, 8, 8]           [32, 128, 8, 8]           --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 128, 8, 8]           [32, 128, 8, 8]           [3, 3]                    147,456\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 128, 8, 8]           [32, 128, 8, 8]           --                        256\n",
       "│    │    └─ReLU (relu)                  [32, 128, 8, 8]           [32, 128, 8, 8]           --                        --\n",
       "├─Sequential (layer3)                    [32, 128, 8, 8]           [32, 256, 4, 4]           --                        --\n",
       "│    └─BasicBlock (0)                    [32, 128, 8, 8]           [32, 256, 4, 4]           --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 128, 8, 8]           [32, 256, 4, 4]           [3, 3]                    294,912\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 256, 4, 4]           [32, 256, 4, 4]           --                        512\n",
       "│    │    └─ReLU (relu)                  [32, 256, 4, 4]           [32, 256, 4, 4]           --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 256, 4, 4]           [32, 256, 4, 4]           [3, 3]                    589,824\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 256, 4, 4]           [32, 256, 4, 4]           --                        512\n",
       "│    │    └─Sequential (downsample)      [32, 128, 8, 8]           [32, 256, 4, 4]           --                        33,280\n",
       "│    │    └─ReLU (relu)                  [32, 256, 4, 4]           [32, 256, 4, 4]           --                        --\n",
       "│    └─BasicBlock (1)                    [32, 256, 4, 4]           [32, 256, 4, 4]           --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 256, 4, 4]           [32, 256, 4, 4]           [3, 3]                    589,824\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 256, 4, 4]           [32, 256, 4, 4]           --                        512\n",
       "│    │    └─ReLU (relu)                  [32, 256, 4, 4]           [32, 256, 4, 4]           --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 256, 4, 4]           [32, 256, 4, 4]           [3, 3]                    589,824\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 256, 4, 4]           [32, 256, 4, 4]           --                        512\n",
       "│    │    └─ReLU (relu)                  [32, 256, 4, 4]           [32, 256, 4, 4]           --                        --\n",
       "├─Sequential (layer4)                    [32, 256, 4, 4]           [32, 512, 2, 2]           --                        --\n",
       "│    └─BasicBlock (0)                    [32, 256, 4, 4]           [32, 512, 2, 2]           --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 256, 4, 4]           [32, 512, 2, 2]           [3, 3]                    1,179,648\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 512, 2, 2]           [32, 512, 2, 2]           --                        1,024\n",
       "│    │    └─ReLU (relu)                  [32, 512, 2, 2]           [32, 512, 2, 2]           --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 512, 2, 2]           [32, 512, 2, 2]           [3, 3]                    2,359,296\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 512, 2, 2]           [32, 512, 2, 2]           --                        1,024\n",
       "│    │    └─Sequential (downsample)      [32, 256, 4, 4]           [32, 512, 2, 2]           --                        132,096\n",
       "│    │    └─ReLU (relu)                  [32, 512, 2, 2]           [32, 512, 2, 2]           --                        --\n",
       "│    └─BasicBlock (1)                    [32, 512, 2, 2]           [32, 512, 2, 2]           --                        --\n",
       "│    │    └─Conv2d (conv1)               [32, 512, 2, 2]           [32, 512, 2, 2]           [3, 3]                    2,359,296\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 512, 2, 2]           [32, 512, 2, 2]           --                        1,024\n",
       "│    │    └─ReLU (relu)                  [32, 512, 2, 2]           [32, 512, 2, 2]           --                        --\n",
       "│    │    └─Conv2d (conv2)               [32, 512, 2, 2]           [32, 512, 2, 2]           [3, 3]                    2,359,296\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 512, 2, 2]           [32, 512, 2, 2]           --                        1,024\n",
       "│    │    └─ReLU (relu)                  [32, 512, 2, 2]           [32, 512, 2, 2]           --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)            [32, 512, 2, 2]           [32, 512, 1, 1]           --                        --\n",
       "├─Linear (fc)                            [32, 512]                 [32, 10]                  --                        5,130\n",
       "============================================================================================================================================\n",
       "Total params: 11,181,642\n",
       "Trainable params: 11,181,642\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.74\n",
       "============================================================================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 103.81\n",
       "Params size (MB): 44.73\n",
       "Estimated Total Size (MB): 150.11\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As table\n",
    "model_stats = torchinfo.summary(\n",
    "    \n",
    "        model, \n",
    "        input_size = (batch_size, 3, 64, 64), \n",
    "        col_names = [\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"],  # \"mult_adds\"\n",
    "        verbose = 0,  # 0: quiet / Jupyter default , 1: default, 2: full detail\n",
    "        row_settings = [\"var_names\"],\n",
    "        depth = 3\n",
    "        \n",
    ")\n",
    "\n",
    "model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "251147f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model graph saved to `resnet18_architecture.pdf`.\n"
     ]
    }
   ],
   "source": [
    "# Trace graph\n",
    "batch_sample = next(iter(train_dataloader))  # Get exemplary single batch\n",
    "input_sample = batch_sample[0]  # Access raw image data sample\n",
    "y = model(input_sample.to(device))  # Forward pass to trace\n",
    "\n",
    "# Render graph and output to PDF\n",
    "graph = torchviz.make_dot(y, params=dict(model.named_parameters()))  # `params` links parameters to nodes\n",
    "graph.render(\"resnet18_architecture\", format=\"pdf\", cleanup=True)\n",
    "print(\"Model graph saved to `resnet18_architecture.pdf`.\")\n",
    "\n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cea550",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fefb8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "-------------------------------\n",
      "Loss: 2.359715, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.835579, Sample: [ 3232/27000], Delta: -0.835579\n",
      "Loss: 0.733612, Sample: [ 6432/27000], Delta: -0.733612\n",
      "Loss: 0.431659, Sample: [ 9632/27000], Delta: -0.431659\n",
      "Loss: 0.551029, Sample: [12832/27000], Delta: -0.551029\n",
      "Loss: 0.188309, Sample: [16032/27000], Delta: -0.188309\n",
      "Loss: 0.433348, Sample: [19232/27000], Delta: -0.433348\n",
      "Loss: 0.509371, Sample: [22432/27000], Delta: -0.509371\n",
      "Loss: 0.321945, Sample: [25632/27000], Delta: -0.321945\n",
      "Average: 0.358431, Accuracy: 87.5%\n",
      "\n",
      "epoch 2\n",
      "-------------------------------\n",
      "Loss: 0.357981, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.571829, Sample: [ 3232/27000], Delta: -0.571829\n",
      "Loss: 0.520238, Sample: [ 6432/27000], Delta: -0.520238\n",
      "Loss: 0.729696, Sample: [ 9632/27000], Delta: -0.729696\n",
      "Loss: 0.351947, Sample: [12832/27000], Delta: -0.351947\n",
      "Loss: 0.408615, Sample: [16032/27000], Delta: -0.408615\n",
      "Loss: 0.326342, Sample: [19232/27000], Delta: -0.326342\n",
      "Loss: 0.159091, Sample: [22432/27000], Delta: -0.159091\n",
      "Loss: 0.302499, Sample: [25632/27000], Delta: -0.302499\n",
      "Average: 0.307987, Accuracy: 89.4%\n",
      "\n",
      "epoch 3\n",
      "-------------------------------\n",
      "Loss: 0.228540, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.315931, Sample: [ 3232/27000], Delta: -0.315931\n",
      "Loss: 0.540960, Sample: [ 6432/27000], Delta: -0.540960\n",
      "Loss: 0.488182, Sample: [ 9632/27000], Delta: -0.488182\n",
      "Loss: 0.187943, Sample: [12832/27000], Delta: -0.187943\n",
      "Loss: 0.209456, Sample: [16032/27000], Delta: -0.209456\n",
      "Loss: 0.106139, Sample: [19232/27000], Delta: -0.106139\n",
      "Loss: 0.127323, Sample: [22432/27000], Delta: -0.127323\n",
      "Loss: 0.278323, Sample: [25632/27000], Delta: -0.278323\n",
      "Average: 0.292120, Accuracy: 90.1%\n",
      "\n",
      "epoch 4\n",
      "-------------------------------\n",
      "Loss: 0.692132, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.223726, Sample: [ 3232/27000], Delta: -0.223726\n",
      "Loss: 0.264983, Sample: [ 6432/27000], Delta: -0.264983\n",
      "Loss: 0.482392, Sample: [ 9632/27000], Delta: -0.482392\n",
      "Loss: 0.399925, Sample: [12832/27000], Delta: -0.399925\n",
      "Loss: 0.320717, Sample: [16032/27000], Delta: -0.320717\n",
      "Loss: 0.329266, Sample: [19232/27000], Delta: -0.329266\n",
      "Loss: 0.230898, Sample: [22432/27000], Delta: -0.230898\n",
      "Loss: 0.186920, Sample: [25632/27000], Delta: -0.186920\n",
      "Average: 0.261437, Accuracy: 91.1%\n",
      "\n",
      "epoch 5\n",
      "-------------------------------\n",
      "Loss: 0.192308, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.423764, Sample: [ 3232/27000], Delta: -0.423764\n",
      "Loss: 0.063365, Sample: [ 6432/27000], Delta: -0.063365\n",
      "Loss: 0.136570, Sample: [ 9632/27000], Delta: -0.136570\n",
      "Loss: 0.279824, Sample: [12832/27000], Delta: -0.279824\n",
      "Loss: 0.376094, Sample: [16032/27000], Delta: -0.376094\n",
      "Loss: 0.372563, Sample: [19232/27000], Delta: -0.372563\n",
      "Loss: 0.182963, Sample: [22432/27000], Delta: -0.182963\n",
      "Loss: 0.287782, Sample: [25632/27000], Delta: -0.287782\n",
      "Average: 0.261461, Accuracy: 91.0%\n",
      "\n",
      "epoch 6\n",
      "-------------------------------\n",
      "Loss: 0.265596, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.247897, Sample: [ 3232/27000], Delta: -0.247897\n",
      "Loss: 0.153346, Sample: [ 6432/27000], Delta: -0.153346\n",
      "Loss: 0.231499, Sample: [ 9632/27000], Delta: -0.231499\n",
      "Loss: 0.236245, Sample: [12832/27000], Delta: -0.236245\n",
      "Loss: 0.185459, Sample: [16032/27000], Delta: -0.185459\n",
      "Loss: 0.192886, Sample: [19232/27000], Delta: -0.192886\n",
      "Loss: 0.207969, Sample: [22432/27000], Delta: -0.207969\n",
      "Loss: 0.154066, Sample: [25632/27000], Delta: -0.154066\n",
      "Average: 0.211002, Accuracy: 93.1%\n",
      "\n",
      "epoch 7\n",
      "-------------------------------\n",
      "Loss: 0.198361, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.149696, Sample: [ 3232/27000], Delta: -0.149696\n",
      "Loss: 0.196283, Sample: [ 6432/27000], Delta: -0.196283\n",
      "Loss: 0.169272, Sample: [ 9632/27000], Delta: -0.169272\n",
      "Loss: 0.418010, Sample: [12832/27000], Delta: -0.418010\n",
      "Loss: 0.442276, Sample: [16032/27000], Delta: -0.442276\n",
      "Loss: 0.278856, Sample: [19232/27000], Delta: -0.278856\n",
      "Loss: 0.194680, Sample: [22432/27000], Delta: -0.194680\n",
      "Loss: 0.085292, Sample: [25632/27000], Delta: -0.085292\n",
      "Average: 0.197285, Accuracy: 93.2%\n",
      "\n",
      "epoch 8\n",
      "-------------------------------\n",
      "Loss: 0.544269, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.264738, Sample: [ 3232/27000], Delta: -0.264738\n",
      "Loss: 0.123907, Sample: [ 6432/27000], Delta: -0.123907\n",
      "Loss: 0.125852, Sample: [ 9632/27000], Delta: -0.125852\n",
      "Loss: 0.230710, Sample: [12832/27000], Delta: -0.230710\n",
      "Loss: 0.268801, Sample: [16032/27000], Delta: -0.268801\n",
      "Loss: 0.199838, Sample: [19232/27000], Delta: -0.199838\n",
      "Loss: 0.206714, Sample: [22432/27000], Delta: -0.206714\n",
      "Loss: 0.258881, Sample: [25632/27000], Delta: -0.258881\n",
      "Average: 0.222827, Accuracy: 92.1%\n",
      "\n",
      "epoch 9\n",
      "-------------------------------\n",
      "Loss: 0.178362, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.418434, Sample: [ 3232/27000], Delta: -0.418434\n",
      "Loss: 0.371056, Sample: [ 6432/27000], Delta: -0.371056\n",
      "Loss: 0.095922, Sample: [ 9632/27000], Delta: -0.095922\n",
      "Loss: 0.220694, Sample: [12832/27000], Delta: -0.220694\n",
      "Loss: 0.159223, Sample: [16032/27000], Delta: -0.159223\n",
      "Loss: 0.161429, Sample: [19232/27000], Delta: -0.161429\n",
      "Loss: 0.226917, Sample: [22432/27000], Delta: -0.226917\n",
      "Loss: 0.568512, Sample: [25632/27000], Delta: -0.568512\n",
      "Average: 0.205624, Accuracy: 92.9%\n",
      "\n",
      "epoch 10\n",
      "-------------------------------\n",
      "Loss: 0.351166, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.255359, Sample: [ 3232/27000], Delta: -0.255359\n",
      "Loss: 0.298073, Sample: [ 6432/27000], Delta: -0.298073\n",
      "Loss: 0.165364, Sample: [ 9632/27000], Delta: -0.165364\n",
      "Loss: 0.113840, Sample: [12832/27000], Delta: -0.113840\n",
      "Loss: 0.217531, Sample: [16032/27000], Delta: -0.217531\n",
      "Loss: 0.308102, Sample: [19232/27000], Delta: -0.308102\n",
      "Loss: 0.502683, Sample: [22432/27000], Delta: -0.502683\n",
      "Loss: 0.254497, Sample: [25632/27000], Delta: -0.254497\n",
      "Average: 0.189400, Accuracy: 93.6%\n",
      "\n",
      "epoch 11\n",
      "-------------------------------\n",
      "Loss: 0.158061, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.236930, Sample: [ 3232/27000], Delta: -0.236930\n",
      "Loss: 0.213320, Sample: [ 6432/27000], Delta: -0.213320\n",
      "Loss: 0.125427, Sample: [ 9632/27000], Delta: -0.125427\n",
      "Loss: 0.114995, Sample: [12832/27000], Delta: -0.114995\n",
      "Loss: 0.289735, Sample: [16032/27000], Delta: -0.289735\n",
      "Loss: 0.089255, Sample: [19232/27000], Delta: -0.089255\n",
      "Loss: 0.443996, Sample: [22432/27000], Delta: -0.443996\n",
      "Loss: 0.229807, Sample: [25632/27000], Delta: -0.229807\n",
      "Average: 0.203503, Accuracy: 93.0%\n",
      "\n",
      "epoch 12\n",
      "-------------------------------\n",
      "Loss: 0.230960, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.061875, Sample: [ 3232/27000], Delta: -0.061875\n",
      "Loss: 0.080503, Sample: [ 6432/27000], Delta: -0.080503\n",
      "Loss: 0.579375, Sample: [ 9632/27000], Delta: -0.579375\n",
      "Loss: 0.358511, Sample: [12832/27000], Delta: -0.358511\n",
      "Loss: 0.286655, Sample: [16032/27000], Delta: -0.286655\n",
      "Loss: 0.095108, Sample: [19232/27000], Delta: -0.095108\n",
      "Loss: 0.162984, Sample: [22432/27000], Delta: -0.162984\n",
      "Loss: 0.166506, Sample: [25632/27000], Delta: -0.166506\n",
      "Average: 0.183597, Accuracy: 93.7%\n",
      "\n",
      "epoch 13\n",
      "-------------------------------\n",
      "Loss: 0.165343, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.212378, Sample: [ 3232/27000], Delta: -0.212378\n",
      "Loss: 0.165422, Sample: [ 6432/27000], Delta: -0.165422\n",
      "Loss: 0.300848, Sample: [ 9632/27000], Delta: -0.300848\n",
      "Loss: 0.182893, Sample: [12832/27000], Delta: -0.182893\n",
      "Loss: 0.282296, Sample: [16032/27000], Delta: -0.282296\n",
      "Loss: 0.186986, Sample: [19232/27000], Delta: -0.186986\n",
      "Loss: 0.142138, Sample: [22432/27000], Delta: -0.142138\n",
      "Loss: 0.155368, Sample: [25632/27000], Delta: -0.155368\n",
      "Average: 0.198335, Accuracy: 93.2%\n",
      "\n",
      "epoch 14\n",
      "-------------------------------\n",
      "Loss: 0.123162, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.030455, Sample: [ 3232/27000], Delta: -0.030455\n",
      "Loss: 0.299161, Sample: [ 6432/27000], Delta: -0.299161\n",
      "Loss: 0.157180, Sample: [ 9632/27000], Delta: -0.157180\n",
      "Loss: 0.489707, Sample: [12832/27000], Delta: -0.489707\n",
      "Loss: 0.147167, Sample: [16032/27000], Delta: -0.147167\n",
      "Loss: 0.154674, Sample: [19232/27000], Delta: -0.154674\n",
      "Loss: 0.076643, Sample: [22432/27000], Delta: -0.076643\n",
      "Loss: 0.252537, Sample: [25632/27000], Delta: -0.252537\n",
      "Average: 0.169830, Accuracy: 94.1%\n",
      "\n",
      "epoch 15\n",
      "-------------------------------\n",
      "Loss: 0.153204, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.153359, Sample: [ 3232/27000], Delta: -0.153359\n",
      "Loss: 0.139280, Sample: [ 6432/27000], Delta: -0.139280\n",
      "Loss: 0.108810, Sample: [ 9632/27000], Delta: -0.108810\n",
      "Loss: 0.178589, Sample: [12832/27000], Delta: -0.178589\n",
      "Loss: 0.131344, Sample: [16032/27000], Delta: -0.131344\n",
      "Loss: 0.199220, Sample: [19232/27000], Delta: -0.199220\n",
      "Loss: 0.094209, Sample: [22432/27000], Delta: -0.094209\n",
      "Loss: 0.198600, Sample: [25632/27000], Delta: -0.198600\n",
      "Average: 0.170812, Accuracy: 94.2%\n",
      "\n",
      "epoch 16\n",
      "-------------------------------\n",
      "Loss: 0.059972, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.141487, Sample: [ 3232/27000], Delta: -0.141487\n",
      "Loss: 0.092131, Sample: [ 6432/27000], Delta: -0.092131\n",
      "Loss: 0.237119, Sample: [ 9632/27000], Delta: -0.237119\n",
      "Loss: 0.432694, Sample: [12832/27000], Delta: -0.432694\n",
      "Loss: 0.071982, Sample: [16032/27000], Delta: -0.071982\n",
      "Loss: 0.135704, Sample: [19232/27000], Delta: -0.135704\n",
      "Loss: 0.212315, Sample: [22432/27000], Delta: -0.212315\n",
      "Loss: 0.458636, Sample: [25632/27000], Delta: -0.458636\n",
      "Average: 0.161687, Accuracy: 94.5%\n",
      "\n",
      "epoch 17\n",
      "-------------------------------\n",
      "Loss: 0.050090, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.167134, Sample: [ 3232/27000], Delta: -0.167134\n",
      "Loss: 0.041422, Sample: [ 6432/27000], Delta: -0.041422\n",
      "Loss: 0.132868, Sample: [ 9632/27000], Delta: -0.132868\n",
      "Loss: 0.217464, Sample: [12832/27000], Delta: -0.217464\n",
      "Loss: 0.184319, Sample: [16032/27000], Delta: -0.184319\n",
      "Loss: 0.257299, Sample: [19232/27000], Delta: -0.257299\n",
      "Loss: 0.132609, Sample: [22432/27000], Delta: -0.132609\n",
      "Loss: 0.093330, Sample: [25632/27000], Delta: -0.093330\n",
      "Average: 0.153608, Accuracy: 94.9%\n",
      "\n",
      "epoch 18\n",
      "-------------------------------\n",
      "Loss: 0.121982, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.246515, Sample: [ 3232/27000], Delta: -0.246515\n",
      "Loss: 0.140598, Sample: [ 6432/27000], Delta: -0.140598\n",
      "Loss: 0.077641, Sample: [ 9632/27000], Delta: -0.077641\n",
      "Loss: 0.596776, Sample: [12832/27000], Delta: -0.596776\n",
      "Loss: 0.095277, Sample: [16032/27000], Delta: -0.095277\n",
      "Loss: 0.238826, Sample: [19232/27000], Delta: -0.238826\n",
      "Loss: 0.142530, Sample: [22432/27000], Delta: -0.142530\n",
      "Loss: 0.192638, Sample: [25632/27000], Delta: -0.192638\n",
      "Average: 0.140351, Accuracy: 94.9%\n",
      "\n",
      "epoch 19\n",
      "-------------------------------\n",
      "Loss: 0.085345, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.372805, Sample: [ 3232/27000], Delta: -0.372805\n",
      "Loss: 0.068447, Sample: [ 6432/27000], Delta: -0.068447\n",
      "Loss: 0.216646, Sample: [ 9632/27000], Delta: -0.216646\n",
      "Loss: 0.188661, Sample: [12832/27000], Delta: -0.188661\n",
      "Loss: 0.038735, Sample: [16032/27000], Delta: -0.038735\n",
      "Loss: 0.232590, Sample: [19232/27000], Delta: -0.232590\n",
      "Loss: 0.192455, Sample: [22432/27000], Delta: -0.192455\n",
      "Loss: 0.237154, Sample: [25632/27000], Delta: -0.237154\n",
      "Average: 0.130116, Accuracy: 95.8%\n",
      "\n",
      "epoch 20\n",
      "-------------------------------\n",
      "Loss: 0.050226, Sample: [   32/27000], Delta: 0.000000\n",
      "Loss: 0.065043, Sample: [ 3232/27000], Delta: -0.065043\n",
      "Loss: 0.055837, Sample: [ 6432/27000], Delta: -0.055837\n",
      "Loss: 0.382659, Sample: [ 9632/27000], Delta: -0.382659\n",
      "Loss: 0.063472, Sample: [12832/27000], Delta: -0.063472\n",
      "Loss: 0.121592, Sample: [16032/27000], Delta: -0.121592\n",
      "Loss: 0.021908, Sample: [19232/27000], Delta: -0.021908\n",
      "Loss: 0.196809, Sample: [22432/27000], Delta: -0.196809\n",
      "Loss: 0.170815, Sample: [25632/27000], Delta: -0.170815\n",
      "Average: 0.141216, Accuracy: 95.1%\n",
      "\n",
      "Training Done!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(f\"epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    model.train()  # Initialize \"Training Mode\"\n",
    "\n",
    "    t_epoch_start = time.time()\n",
    "\n",
    "    last_loss = 0.0\n",
    "    \n",
    "    for batch, (img, lbl) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Training on device (GPU/CUDA if available)\n",
    "        img = img.to(device)\n",
    "        lbl = lbl.to(device)  \n",
    " \n",
    "        p = model(img)  # Forward pass\n",
    "        loss = loss_fn(p, lbl)  # Prediction loss\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss logging\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            loss = loss.item()\n",
    "            current_sample_abs = batch * batch_size + len(img)\n",
    "            current_sample_pct = (current_sample_abs / train_size) * 100\n",
    "\n",
    "            # Compute incremental loss difference over iterations \n",
    "            delta_loss = (last_loss - loss) if batch > 0 else 0.0  # Handle first batch\n",
    "\n",
    "            loss_prev = last_loss\n",
    "\n",
    "            print(f\"sample: {current_sample_abs:>5d}/{train_size:>5d} ({current_sample_pct:>0.2f}%) | loss: {loss:>7f} | dl: {delta_loss:>7f}\")\n",
    "\n",
    "\n",
    "    t_epoch_end = time.time()\n",
    "    t_epoch_duration = t_epoch_end - t_epoch_start\n",
    "    \n",
    "    # TEST\n",
    "\n",
    "    model.eval()  # Initialize \"Evaluation Mode\"\n",
    "    \n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    batch_count = len(test_dataloader)\n",
    "\n",
    "    cml_loss = 0.0\n",
    "    correct_predictions = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Avoid gradient unnecessary computation in eval mode\n",
    "\n",
    "        for img, lbl in test_dataloader:\n",
    "\n",
    "            img = img.to(device)\n",
    "            lbl = lbl.to(device)  \n",
    "\n",
    "            p = model(img)  # Forward pass\n",
    "            loss = loss_fn(p, lbl)  # Prediction loss\n",
    "\n",
    "            cml_loss += loss.item()\n",
    "            correct_predictions += (p.argmax(1) == lbl).type(torch.float).sum().item()\n",
    "\n",
    "    avg_loss = cml_loss / batch_count  #  Average loss\n",
    "    model_accuracy = 100*(correct_predictions / test_size)\n",
    "\n",
    "    print(f\"average: {avg_loss:>8f}, accuracy: {model_accuracy:>0.1f}%, train time: {t_epoch_duration:>0.1f}\\n\")\n",
    "\n",
    "    # print(f\"\\n\")\n",
    "\n",
    "print(\"Training Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfca637",
   "metadata": {},
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d60e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as PyTorch state dictionary\n",
    "torch.save(model.state_dict(), 'results/models/quick_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
